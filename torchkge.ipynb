{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "from google.colab import drive\ndrive.mount('/content/drive')",
   "metadata": {
    "id": "8PJFJBC9mhgB",
    "outputId": "749c17f8-9935-40fe-9983-db5d42efb032"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "cd /content/drive/Shareddrives/xm/torchkge",
   "metadata": {
    "id": "TYXsPvGemqsU",
    "outputId": "d5f96bb9-7521-4cc0-d697-9c36719da91f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python3 test.py",
   "metadata": {
    "id": "psiTFXNA7D3K",
    "outputId": "00e3de41-9e9b-4646-d4fc-29fb9ba9ae4f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python3 test.py",
   "metadata": {
    "id": "68SK6A-Gj-WS",
    "outputId": "1c1264cb-3618-4b0c-fe42-828394f8e7e7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python3 test.py",
   "metadata": {
    "id": "Flrwq44uqStZ",
    "outputId": "df3674d7-86a5-4df4-993c-636ce00c7239"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python3 test.py",
   "metadata": {
    "id": "xFruK6LbqpsY",
    "outputId": "91faca02-d8c5-4385-aee3-439834c319da"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/Moon-xm/torchkge.git",
   "metadata": {
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'torchkge'...\nremote: Enumerating objects: 137, done.\u001B[K\nremote: Counting objects: 100% (137/137), done.\u001B[K\nremote: Compressing objects: 100% (90/90), done.\u001B[K\nremote: Total 3458 (delta 71), reused 101 (delta 46), pack-reused 3321\u001B[K\nReceiving objects: 100% (3458/3458), 64.76 MiB | 22.30 MiB/s, done.\nResolving deltas: 100% (2273/2273), done.\nChecking out files: 100% (91/91), done.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "cd torchkge",
   "metadata": {
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/torchkge\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!ls",
   "metadata": {
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "GADM9_TransD.py\t\tGeoDBpedia21_TransR.py\tdocs\t\t      tests\nGADM9_TransE.py\t\tLICENSE\t\t\trequirements_dev.txt  torchkge\nGADM9_TransE_GDR.py\tREADME.rst\t\tsetup.cfg\t      tox.ini\nGADM9_TransR.py\t\tbenchmarks\t\tsetup.py\nGeoDBpedia21_TransD.py\tcheckpoint\t\ttest_ignite.py\nGeoDBpedia21_TransE.py\tdataprocess.py\t\ttest_short.py\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!python3 GADM9_TransE.py",
   "metadata": {
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nloading ckpt sucessful...\nTransEModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n)\nlr: 0.0001, margin: 1.5, dim 100, total epoch: 2000, device: cuda:0, batch size: 5120,optim: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.0001\n    weight_decay: 1e-05\n)\nTraining ...\nEpoch [1200/2000] | mean loss:   50.483, time: 0:00:43\nTrain loss:   50.483, Val Hit@10: 70.75%, Time 0:01:24 \nEpoch [1267/2000] | mean loss:   49.089, time: 0:01:53^C\nTraceback (most recent call last):\n  File \"GADM9_TransE.py\", line 135, in <module>\n    main()\n  File \"GADM9_TransE.py\", line 95, in main\n    loss.backward()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/tensor.py\", line 221, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 132, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nKeyboardInterrupt\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "GADM9_TransE_Adam.py",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam, SGD\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransE'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    lr = 0.0001\n    margin = 1.5\n\n    n_epochs = 5000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 512  # 测评valid test 时batch size\n    validation_freq = 200 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = 500  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n#     optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    optimiezer = SGD(model.parameters(), lr=lr)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {},optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10(filter): {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"\\nNo optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nloading ckpt sucessful, start on epoch 1101...\nTransEModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n)\nlr: 0.0001, margin: 1.5, dim 100, total epoch: 5000, device: cuda:0, batch size: 5120,optim: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.0001\n    weight_decay: 1e-05\n)\nTraining ...\nEpoch [1200/5000] | mean loss:   50.586, time: 0:00:43\nTrain loss:   50.586, Val Hit@10: 70.87%, Time 0:01:24 \nEpoch [1400/5000] | mean loss:   46.193, time: 0:02:49\nTrain loss:   46.193, Val Hit@10: 69.42%, Time 0:03:30 \nEpoch [1600/5000] | mean loss:   44.247, time: 0:04:54\nTrain loss:   44.247, Val Hit@10: 68.68%, Time 0:05:35 \nEpoch [1602/5000] | mean loss:   44.138, time: 0:05:36No optimization for a long time, auto-stopping...\nTraining done, start evaluate on test data...\nHit@10 : 0.606 \t\t Filt. Hit@10 : 0.724\nMean Rank : 841 \t Filt. Mean Rank : 664\nMRR : 0.351 \t\t Filt. MRR : 0.485\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "SGD",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam, SGD\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransE'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    lr = 0.0001\n    margin = 1.5\n\n    n_epochs = 5000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 512  # 测评valid test 时batch size\n    validation_freq = 200 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = 1000  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '_SGD.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n#     optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    optimizer = SGD(model.parameters(), lr=lr)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {},optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10(filter): {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"No optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nTransEModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n)\nlr: 0.0001, margin: 1.5, dim 100, total epoch: 5000, device: cuda:0, batch size: 5120,optim: SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.0001\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\nTraining ...\nEpoch [ 200/5000] | mean loss: 1501.833, time: 0:01:17\nTrain loss: 1501.833, Val Hit@10(filter): 9.77%, Time 0:01:58 *\nEpoch [ 400/5000] | mean loss: 1343.400, time: 0:03:14\nTrain loss: 1343.400, Val Hit@10(filter): 9.96%, Time 0:03:55 *\nEpoch [ 600/5000] | mean loss: 1212.495, time: 0:05:12\nTrain loss: 1212.495, Val Hit@10(filter): 10.21%, Time 0:05:53 *\nEpoch [ 800/5000] | mean loss: 1070.639, time: 0:07:10\nTrain loss: 1070.639, Val Hit@10(filter): 10.53%, Time 0:07:51 *\nEpoch [1000/5000] | mean loss:  950.793, time: 0:09:08\nTrain loss:  950.793, Val Hit@10(filter): 11.00%, Time 0:09:49 *\nEpoch [1200/5000] | mean loss:  823.963, time: 0:11:06\nTrain loss:  823.963, Val Hit@10(filter): 11.63%, Time 0:11:47 *\nEpoch [1400/5000] | mean loss:  715.257, time: 0:13:04\nTrain loss:  715.257, Val Hit@10(filter): 12.63%, Time 0:13:45 *\nEpoch [1600/5000] | mean loss:  619.255, time: 0:15:02\nTrain loss:  619.255, Val Hit@10(filter): 14.07%, Time 0:15:43 *\nEpoch [1800/5000] | mean loss:  529.557, time: 0:17:00\nTrain loss:  529.557, Val Hit@10(filter): 16.09%, Time 0:17:40 *\nEpoch [2000/5000] | mean loss:  460.102, time: 0:18:57\nTrain loss:  460.102, Val Hit@10(filter): 18.76%, Time 0:19:38 *\nEpoch [2200/5000] | mean loss:  399.391, time: 0:20:55\nTrain loss:  399.391, Val Hit@10(filter): 22.14%, Time 0:21:36 *\nEpoch [2400/5000] | mean loss:  351.244, time: 0:22:53\nTrain loss:  351.244, Val Hit@10(filter): 25.92%, Time 0:23:33 *\nEpoch [2600/5000] | mean loss:  313.000, time: 0:24:50\nTrain loss:  313.000, Val Hit@10(filter): 29.77%, Time 0:25:31 *\nEpoch [2800/5000] | mean loss:  275.408, time: 0:26:48\nTrain loss:  275.408, Val Hit@10(filter): 33.19%, Time 0:27:29 *\nEpoch [3000/5000] | mean loss:  250.757, time: 0:28:46\nTrain loss:  250.757, Val Hit@10(filter): 36.31%, Time 0:29:27 *\nEpoch [3200/5000] | mean loss:  228.925, time: 0:30:44\nTrain loss:  228.925, Val Hit@10(filter): 39.11%, Time 0:31:25 *\nEpoch [3400/5000] | mean loss:  207.978, time: 0:32:42\nTrain loss:  207.978, Val Hit@10(filter): 41.33%, Time 0:33:23 *\nEpoch [3600/5000] | mean loss:  193.557, time: 0:34:39\nTrain loss:  193.557, Val Hit@10(filter): 43.04%, Time 0:35:20 *\nEpoch [3800/5000] | mean loss:  181.028, time: 0:36:37\nTrain loss:  181.028, Val Hit@10(filter): 44.56%, Time 0:37:18 *\nEpoch [4000/5000] | mean loss:  167.257, time: 0:38:35\nTrain loss:  167.257, Val Hit@10(filter): 46.09%, Time 0:39:16 *\nEpoch [4200/5000] | mean loss:  160.588, time: 0:40:33\nTrain loss:  160.588, Val Hit@10(filter): 47.45%, Time 0:41:14 *\nEpoch [4400/5000] | mean loss:  149.199, time: 0:42:31\nTrain loss:  149.199, Val Hit@10(filter): 48.37%, Time 0:43:12 *\nEpoch [4600/5000] | mean loss:  143.862, time: 0:44:29\nTrain loss:  143.862, Val Hit@10(filter): 49.31%, Time 0:45:10 *\nEpoch [4800/5000] | mean loss:  137.154, time: 0:46:27\nTrain loss:  137.154, Val Hit@10(filter): 50.12%, Time 0:47:07 *\nEpoch [5000/5000] | mean loss:  131.143, time: 0:48:24\nTrain loss:  131.143, Val Hit@10(filter): 50.98%, Time 0:49:05 *\nTraining done, start evaluate on test data...\nHit@10 : 0.424 \t\t Filt. Hit@10 : 0.51\nMean Rank : 3148 \t Filt. Mean Rank : 2969\nMRR : 0.185 \t\t Filt. MRR : 0.249\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam, SGD\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransE'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    lr = 0.0001\n    margin = 1.5\n\n    n_epochs = 20000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 512  # 测评valid test 时batch size\n    validation_freq = 500 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = 2000  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '_SGD.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n#     optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    optimizer = SGD(model.parameters(), lr=lr)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {},optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10(filter): {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"No optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nloading ckpt sucessful, start on epoch 5001...\nTransEModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n)\nlr: 0.0001, margin: 1.5, dim 100, total epoch: 20000, device: cuda:0, batch size: 5120,optim: SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.0001\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\nTraining ...\nEpoch [5500/20000] | mean loss:  118.078, time: 0:03:12\nTrain loss:  118.078, Val Hit@10(filter): 52.42%, Time 0:03:53 *\nEpoch [6000/20000] | mean loss:  107.632, time: 0:07:05\nTrain loss:  107.632, Val Hit@10(filter): 53.82%, Time 0:07:46 *\nEpoch [6500/20000] | mean loss:  100.063, time: 0:10:58\nTrain loss:  100.063, Val Hit@10(filter): 54.80%, Time 0:11:39 *\nEpoch [7000/20000] | mean loss:   93.712, time: 0:14:52\nTrain loss:   93.712, Val Hit@10(filter): 55.55%, Time 0:15:33 *\nEpoch [7500/20000] | mean loss:   87.426, time: 0:18:46\nTrain loss:   87.426, Val Hit@10(filter): 56.22%, Time 0:19:27 *\nEpoch [8000/20000] | mean loss:   83.081, time: 0:22:38\nTrain loss:   83.081, Val Hit@10(filter): 56.85%, Time 0:23:19 *\nEpoch [8500/20000] | mean loss:   79.228, time: 0:26:31\nTrain loss:   79.228, Val Hit@10(filter): 57.35%, Time 0:27:12 *\nEpoch [9000/20000] | mean loss:   73.198, time: 0:30:25\nTrain loss:   73.198, Val Hit@10(filter): 57.60%, Time 0:31:06 *\nEpoch [9500/20000] | mean loss:   71.996, time: 0:34:19\nTrain loss:   71.996, Val Hit@10(filter): 58.18%, Time 0:35:00 *\nEpoch [10000/20000] | mean loss:   70.014, time: 0:38:13\nTrain loss:   70.014, Val Hit@10(filter): 58.68%, Time 0:38:54 *\nEpoch [10500/20000] | mean loss:   67.341, time: 0:42:06\nTrain loss:   67.341, Val Hit@10(filter): 59.14%, Time 0:42:47 *\nEpoch [11000/20000] | mean loss:   65.130, time: 0:46:01\nTrain loss:   65.130, Val Hit@10(filter): 59.33%, Time 0:46:41 *\nEpoch [11500/20000] | mean loss:   62.045, time: 0:49:54\nTrain loss:   62.045, Val Hit@10(filter): 59.58%, Time 0:50:35 *\nEpoch [12000/20000] | mean loss:   59.489, time: 0:53:47\nTrain loss:   59.489, Val Hit@10(filter): 59.99%, Time 0:54:28 *\nEpoch [12500/20000] | mean loss:   59.793, time: 0:57:40\nTrain loss:   59.793, Val Hit@10(filter): 60.55%, Time 0:58:21 *\nEpoch [13000/20000] | mean loss:   57.977, time: 1:01:34\nTrain loss:   57.977, Val Hit@10(filter): 60.60%, Time 1:02:15 *\nEpoch [13500/20000] | mean loss:   55.755, time: 1:05:30\nTrain loss:   55.755, Val Hit@10(filter): 61.00%, Time 1:06:11 *\nEpoch [14000/20000] | mean loss:   54.893, time: 1:09:24\nTrain loss:   54.893, Val Hit@10(filter): 61.19%, Time 1:10:05 *\nEpoch [14500/20000] | mean loss:   54.497, time: 1:13:17\nTrain loss:   54.497, Val Hit@10(filter): 61.60%, Time 1:13:58 *\nEpoch [15000/20000] | mean loss:   52.342, time: 1:17:12\nTrain loss:   52.342, Val Hit@10(filter): 61.71%, Time 1:17:53 *\nEpoch [15500/20000] | mean loss:   52.193, time: 1:21:05\nTrain loss:   52.193, Val Hit@10(filter): 61.86%, Time 1:21:46 *\nEpoch [16000/20000] | mean loss:   50.280, time: 1:24:58\nTrain loss:   50.280, Val Hit@10(filter): 62.22%, Time 1:25:39 *\nEpoch [16500/20000] | mean loss:   49.978, time: 1:28:50\nTrain loss:   49.978, Val Hit@10(filter): 62.40%, Time 1:29:31 *\nEpoch [17000/20000] | mean loss:   48.450, time: 1:32:43\nTrain loss:   48.450, Val Hit@10(filter): 62.41%, Time 1:33:24 *\nEpoch [17230/20000] | mean loss:   50.156, time: 1:34:53\nTrain loss:   47.535, Val Hit@10(filter): 62.55%, Time 1:37:18 *\nEpoch [18000/20000] | mean loss:   47.893, time: 1:40:31\nTrain loss:   47.893, Val Hit@10(filter): 62.85%, Time 1:41:12 *\nEpoch [18500/20000] | mean loss:   47.619, time: 1:44:24\nTrain loss:   47.619, Val Hit@10(filter): 62.96%, Time 1:45:04 *\nEpoch [19000/20000] | mean loss:   46.324, time: 1:48:16\nTrain loss:   46.324, Val Hit@10(filter): 63.29%, Time 1:48:57 *\nEpoch [19500/20000] | mean loss:   45.743, time: 1:52:10\nTrain loss:   45.743, Val Hit@10(filter): 63.30%, Time 1:52:51 *\nEpoch [20000/20000] | mean loss:   45.497, time: 1:56:03\nTrain loss:   45.497, Val Hit@10(filter): 63.57%, Time 1:56:44 *\nTraining done, start evaluate on test data...\nHit@10 : 0.528 \t\t Filt. Hit@10 : 0.636\nMean Rank : 1532 \t Filt. Mean Rank : 1349\nMRR : 0.286 \t\t Filt. MRR : 0.388\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam, SGD\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransE'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    lr = 0.0001\n    margin = 1.5\n\n    n_epochs = 50000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 512  # 测评valid test 时batch size\n    validation_freq = 500 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = 2000  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '_SGD.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n#     optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    optimizer = SGD(model.parameters(), lr=lr)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {},optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10(filter): {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"No optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nloading ckpt sucessful, start on epoch 20001...\nTransEModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n)\nlr: 0.0001, margin: 1.5, dim 100, total epoch: 50000, device: cuda:0, batch size: 5120,optim: SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.0001\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\nTraining ...\nEpoch [20500/50000] | mean loss:   44.903, time: 0:03:12\nTrain loss:   44.903, Val Hit@10(filter): 63.47%, Time 0:03:53 *\nEpoch [21000/50000] | mean loss:   45.071, time: 0:07:06\nTrain loss:   45.071, Val Hit@10(filter): 63.71%, Time 0:07:47 *\nEpoch [21500/50000] | mean loss:   43.949, time: 0:11:00\nTrain loss:   43.949, Val Hit@10(filter): 63.77%, Time 0:11:40 *\nEpoch [22000/50000] | mean loss:   44.247, time: 0:14:53\nTrain loss:   44.247, Val Hit@10(filter): 64.00%, Time 0:15:34 *\nEpoch [22500/50000] | mean loss:   44.363, time: 0:18:46\nTrain loss:   44.363, Val Hit@10(filter): 64.07%, Time 0:19:26 *\nEpoch [23000/50000] | mean loss:   43.944, time: 0:22:39\nTrain loss:   43.944, Val Hit@10(filter): 64.12%, Time 0:23:20 *\nEpoch [23500/50000] | mean loss:   42.692, time: 0:26:34\nTrain loss:   42.692, Val Hit@10(filter): 64.32%, Time 0:27:15 *\nEpoch [24000/50000] | mean loss:   42.390, time: 0:30:28\nTrain loss:   42.390, Val Hit@10(filter): 64.28%, Time 0:31:09 \nEpoch [24500/50000] | mean loss:   42.244, time: 0:34:21\nTrain loss:   42.244, Val Hit@10(filter): 64.33%, Time 0:35:02 *\nEpoch [25000/50000] | mean loss:   41.647, time: 0:38:14\nTrain loss:   41.647, Val Hit@10(filter): 64.39%, Time 0:38:55 *\nEpoch [25500/50000] | mean loss:   42.626, time: 0:42:07\nTrain loss:   42.626, Val Hit@10(filter): 64.48%, Time 0:42:48 *\nEpoch [26000/50000] | mean loss:   41.321, time: 0:46:00\nTrain loss:   41.321, Val Hit@10(filter): 64.50%, Time 0:46:41 *\nEpoch [26500/50000] | mean loss:   40.675, time: 0:49:53\nTrain loss:   40.675, Val Hit@10(filter): 64.62%, Time 0:50:34 *\nEpoch [27000/50000] | mean loss:   41.337, time: 0:53:45\nTrain loss:   41.337, Val Hit@10(filter): 64.76%, Time 0:54:26 *\nEpoch [27500/50000] | mean loss:   40.694, time: 0:57:38\nTrain loss:   40.694, Val Hit@10(filter): 64.80%, Time 0:58:19 *\nEpoch [28000/50000] | mean loss:   39.904, time: 1:01:31\nTrain loss:   39.904, Val Hit@10(filter): 64.66%, Time 1:02:12 \nEpoch [28500/50000] | mean loss:   40.306, time: 1:05:24\nTrain loss:   40.306, Val Hit@10(filter): 64.93%, Time 1:06:05 *\nEpoch [29000/50000] | mean loss:   39.512, time: 1:09:18\nTrain loss:   39.512, Val Hit@10(filter): 64.90%, Time 1:09:59 \nEpoch [29500/50000] | mean loss:   39.716, time: 1:13:11\nTrain loss:   39.716, Val Hit@10(filter): 64.99%, Time 1:13:52 *\nEpoch [30000/50000] | mean loss:   39.517, time: 1:17:04\nTrain loss:   39.517, Val Hit@10(filter): 64.97%, Time 1:17:45 \nEpoch [30500/50000] | mean loss:   39.301, time: 1:20:57\nTrain loss:   39.301, Val Hit@10(filter): 65.03%, Time 1:21:38 *\nEpoch [31000/50000] | mean loss:   39.581, time: 1:24:50\nTrain loss:   39.581, Val Hit@10(filter): 65.06%, Time 1:25:31 *\nEpoch [31500/50000] | mean loss:   37.430, time: 1:28:43\nTrain loss:   37.430, Val Hit@10(filter): 65.02%, Time 1:29:24 \nEpoch [32000/50000] | mean loss:   38.497, time: 1:32:35\nTrain loss:   38.497, Val Hit@10(filter): 65.14%, Time 1:33:16 *\nEpoch [32500/50000] | mean loss:   38.518, time: 1:36:29\nTrain loss:   38.518, Val Hit@10(filter): 65.10%, Time 1:37:10 \nEpoch [33000/50000] | mean loss:   38.903, time: 1:40:21\nTrain loss:   38.903, Val Hit@10(filter): 65.09%, Time 1:41:02 \nEpoch [33500/50000] | mean loss:   38.911, time: 1:44:13\nTrain loss:   38.911, Val Hit@10(filter): 65.17%, Time 1:44:54 *\nEpoch [34000/50000] | mean loss:   37.138, time: 1:48:06\nTrain loss:   37.138, Val Hit@10(filter): 65.19%, Time 1:48:48 *\nEpoch [34500/50000] | mean loss:   38.165, time: 1:52:00\nTrain loss:   38.165, Val Hit@10(filter): 65.37%, Time 1:52:40 *\nEpoch [35000/50000] | mean loss:   38.267, time: 1:55:52\nTrain loss:   38.267, Val Hit@10(filter): 65.30%, Time 1:56:32 \nEpoch [35500/50000] | mean loss:   37.786, time: 1:59:46\nTrain loss:   37.786, Val Hit@10(filter): 65.47%, Time 2:00:27 *\nEpoch [36000/50000] | mean loss:   37.954, time: 2:03:38\nTrain loss:   37.954, Val Hit@10(filter): 65.38%, Time 2:04:19 \nEpoch [36500/50000] | mean loss:   37.644, time: 2:07:30\nTrain loss:   37.644, Val Hit@10(filter): 65.37%, Time 2:08:11 \nEpoch [37000/50000] | mean loss:   36.947, time: 2:11:22\nTrain loss:   36.947, Val Hit@10(filter): 65.37%, Time 2:12:02 \nEpoch [37500/50000] | mean loss:   38.102, time: 2:15:14\nTrain loss:   38.102, Val Hit@10(filter): 65.33%, Time 2:15:54 \nEpoch [37501/50000] | mean loss:   36.863, time: 2:15:55No optimization for a long time, auto-stopping...\nTraining done, start evaluate on test data...\nHit@10 : 0.541 \t\t Filt. Hit@10 : 0.654\nMean Rank : 1303 \t Filt. Mean Rank : 1117\nMRR : 0.315 \t\t Filt. MRR : 0.424\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "cd ..",
   "metadata": {
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!mkdir torchkge2",
   "metadata": {
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "cd torchkge2",
   "metadata": {
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/torchkge2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/Moon-xm/torchkge.git --branch Geo",
   "metadata": {
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'torchkge'...\nremote: Enumerating objects: 181, done.\u001B[K\nremote: Counting objects: 100% (181/181), done.\u001B[K\nremote: Compressing objects: 100% (115/115), done.\u001B[K\nremote: Total 3502 (delta 98), reused 138 (delta 65), pack-reused 3321\u001B[K\nReceiving objects: 100% (3502/3502), 74.59 MiB | 24.85 MiB/s, done.\nResolving deltas: 100% (2300/2300), done.\nChecking out files: 100% (94/94), done.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "cd torchkge",
   "metadata": {
    "trusted": true
   },
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/torchkge2/torchkge\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### GeoDBpedia21(new)\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam, SGD\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransE'\n    benchmarks = 'GeoDBpedia21'\n\n    emb_dim = 100\n    lr = 0.001\n    margin = 10\n\n    n_epochs = 20000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 128  # 测评valid test 时batch size\n    validation_freq = 500  # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = 2000  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks + '_' + model_name + '.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {},optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10(filter): {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"\\nNo optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 84988 entities, 21 relations, 91217 triplets.\nValid set: 3973 triplets, Test set: 3982 triplets.\nLoading model...\nloading ckpt sucessful, start on epoch 21...\nTransEModel(\n  (ent_emb): Embedding(84988, 100)\n  (rel_emb): Embedding(21, 100)\n)\nlr: 0.001, margin: 10, dim 100, total epoch: 20000, device: cuda:0, batch size: 5120,optim: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.001\n    weight_decay: 1e-05\n)\nTraining ...\nEpoch [ 500/20000] | mean loss: 2350.876, time: 0:01:013\nTrain loss: 2350.876, Val Hit@10: 7.92%, Time 0:01:07 *\nEpoch [1000/20000] | mean loss: 2304.272, time: 0:02:10\nTrain loss: 2304.272, Val Hit@10: 7.90%, Time 0:02:16 \nEpoch [1500/20000] | mean loss: 2347.319, time: 0:03:19\nTrain loss: 2347.319, Val Hit@10: 7.79%, Time 0:03:24 \nEpoch [2000/20000] | mean loss: 2246.656, time: 0:04:28\nTrain loss: 2246.656, Val Hit@10: 8.23%, Time 0:04:34 *\nEpoch [2500/20000] | mean loss: 2280.063, time: 0:05:37\nTrain loss: 2280.063, Val Hit@10: 7.75%, Time 0:05:43 \nEpoch [3000/20000] | mean loss: 2162.819, time: 0:06:46\nTrain loss: 2162.819, Val Hit@10: 7.79%, Time 0:06:52 \nEpoch [3500/20000] | mean loss: 2212.014, time: 0:07:54\nTrain loss: 2212.014, Val Hit@10: 8.51%, Time 0:08:01 *\nEpoch [4000/20000] | mean loss: 2166.793, time: 0:09:04\nTrain loss: 2166.793, Val Hit@10: 8.42%, Time 0:09:09 \nEpoch [4500/20000] | mean loss: 2098.633, time: 0:10:12\nTrain loss: 2098.633, Val Hit@10: 9.54%, Time 0:10:19 *\nEpoch [5000/20000] | mean loss: 2095.658, time: 0:11:22\nTrain loss: 2095.658, Val Hit@10: 8.47%, Time 0:11:27 \nEpoch [5500/20000] | mean loss: 2110.017, time: 0:12:30\nTrain loss: 2110.017, Val Hit@10: 8.60%, Time 0:12:36 \nEpoch [6000/20000] | mean loss: 2057.654, time: 0:13:39\nTrain loss: 2057.654, Val Hit@10: 8.44%, Time 0:13:45 \nEpoch [6500/20000] | mean loss: 2135.912, time: 0:14:48\nTrain loss: 2135.912, Val Hit@10: 9.20%, Time 0:14:53 \nEpoch [6501/20000] | mean loss: 2132.217, time: 0:14:53No optimization for a long time, auto-stopping...\nTraining done, start evaluate on test data...\nHit@10 : 0.097 \t\t Filt. Hit@10 : 0.098\nMean Rank : 10606 \t Filt. Mean Rank : 10512\nMRR : 0.041 \t\t Filt. MRR : 0.041\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam, SGD\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransE'\n    benchmarks = 'GeoDBpedia21'\n\n    emb_dim = 100\n    lr = 0.001\n    margin = 10\n\n    n_epochs = 20000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 128  # 测评valid test 时batch size\n    validation_freq = 500  # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = 2000  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks + '_' + model_name + '_SGD.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    optimizer = SGD(model.parameters(), lr=lr)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n#     scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {},optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n#         scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10(filter): {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"\\nNo optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 84988 entities, 21 relations, 91217 triplets.\nValid set: 3973 triplets, Test set: 3982 triplets.\nLoading model...\nTransEModel(\n  (ent_emb): Embedding(84988, 100)\n  (rel_emb): Embedding(21, 100)\n)\nlr: 0.001, margin: 10, dim 100, total epoch: 20000, device: cuda:0, batch size: 5120,optim: SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.001\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\nTraining ...\nEpoch [ 500/20000] | mean loss: 2398.119, time: 0:00:522\nTrain loss: 2398.119, Val Hit@10(filter): 9.25%, Time 0:00:58 *\nEpoch [1000/20000] | mean loss: 2185.000, time: 0:01:49\nTrain loss: 2185.000, Val Hit@10(filter): 8.21%, Time 0:01:55 \nEpoch [1500/20000] | mean loss: 2063.176, time: 0:02:46\nTrain loss: 2063.176, Val Hit@10(filter): 8.83%, Time 0:02:52 \nEpoch [2000/20000] | mean loss: 2096.423, time: 0:03:43\nTrain loss: 2096.423, Val Hit@10(filter): 8.44%, Time 0:03:49 \nEpoch [2500/20000] | mean loss: 2014.447, time: 0:04:40\nTrain loss: 2014.447, Val Hit@10(filter): 8.02%, Time 0:04:46 \nEpoch [2501/20000] | mean loss: 2109.607, time: 0:04:46\nNo optimization for a long time, auto-stopping...\nTraining done, start evaluate on test data...\nHit@10 : 0.091 \t\t Filt. Hit@10 : 0.093\nMean Rank : 10021 \t Filt. Mean Rank : 9928\nMRR : 0.033 \t\t Filt. MRR : 0.034\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### GADM9",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### TransR",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransR'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    ent_dim = emb_dim\n    rel_dim = emb_dim\n    lr = 0.0001\n    margin = 1\n\n    n_epochs = 20000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 128  # 测评valid test 时batch size\n    validation_freq = 200 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = validation_freq*5  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '_Adam.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(ent_dim, rel_dim, kg_train.n_ent, kg_train.n_rel)\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {}, optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10: {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"\\nNo optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nloading ckpt sucessful, start on epoch 21...\nTransRModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n  (proj_mat): Embedding(9, 10000)\n)\nlr: 0.0001, margin: 1, dim 100, total epoch: 20000, device: cuda:0, batch size: 5120, optim: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.0001\n    weight_decay: 1e-05\n)\nTraining ...\nEpoch [ 200/20000] | mean loss:  345.520, time: 0:04:24",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40a7a1ce247047798303a0b38dde1dbc"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:  345.520, Val Hit@10: 9.34%, Time 0:05:16 *\nEpoch [ 400/20000] | mean loss:   70.681, time: 0:10:10",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "580a6fef49b74919b42662f5ce36360b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   70.681, Val Hit@10: 14.42%, Time 0:11:02 *\nEpoch [ 600/20000] | mean loss:   30.290, time: 0:15:56",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3c62638ddb2444290f47023b1a74dd3"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   30.290, Val Hit@10: 19.28%, Time 0:16:47 *\nEpoch [ 800/20000] | mean loss:   16.846, time: 0:21:42",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd64958b320b4057892f4eafdd86bee5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   16.846, Val Hit@10: 24.14%, Time 0:22:34 *\nEpoch [1000/20000] | mean loss:   11.951, time: 0:27:28",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0265956e11fd4967b3cd33a5adfb6272"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "cd torchkge",
   "metadata": {
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/torchkge\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransR'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    ent_dim = emb_dim\n    rel_dim = emb_dim\n    lr = 0.0001\n    margin = 1\n\n    n_epochs = 20000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 128  # 测评valid test 时batch size\n    validation_freq = 200 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = validation_freq*5  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '_Adam.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(ent_dim, rel_dim, kg_train.n_ent, kg_train.n_rel)\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {}, optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10: {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"\\nNo optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nTransRModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n  (proj_mat): Embedding(9, 10000)\n)\nlr: 0.0001, margin: 1, dim 100, total epoch: 20000, device: cuda:0, batch size: 5120, optim: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.0001\n    weight_decay: 1e-05\n)\nTraining ...\nEpoch [ 200/20000] | mean loss:  382.378, time: 0:05:23",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5588a0d56f940eba4314541e02c6009"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:  382.378, Val Hit@10: 9.17%, Time 0:06:26 *\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py:184: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  if p.grad is not None:\n/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py:78: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  if p.grad is not None:\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch [ 400/20000] | mean loss:   64.926, time: 0:11:50",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "651996536396479fb7dd0a2e44c5fdbf"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   64.926, Val Hit@10: 14.47%, Time 0:12:53 *\nEpoch [ 600/20000] | mean loss:   27.792, time: 0:18:16",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb57ad5287f44fd49015c72f364c60b4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   27.792, Val Hit@10: 19.90%, Time 0:19:19 *\nEpoch [ 800/20000] | mean loss:   15.344, time: 0:24:40",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce980d3ebd074151b43d8ee0927789db"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   15.344, Val Hit@10: 25.34%, Time 0:25:44 *\nEpoch [1000/20000] | mean loss:   10.119, time: 0:31:05",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a61ab044948c410b8d0d1ce3f4522f5a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:   10.119, Val Hit@10: 30.23%, Time 0:32:08 *\nEpoch [1200/20000] | mean loss:    8.347, time: 0:37:29",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e64230abc9e442dfb9b4dd9524a4039c"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    8.347, Val Hit@10: 34.27%, Time 0:38:32 *\nEpoch [1400/20000] | mean loss:    6.529, time: 0:43:52",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fcbfbd07bba4f0cae2fe34186f10b6a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    6.529, Val Hit@10: 37.42%, Time 0:44:55 *\nEpoch [1600/20000] | mean loss:    4.948, time: 0:50:17",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "126258a8211942d8be20983a13791d90"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    4.948, Val Hit@10: 39.74%, Time 0:51:20 *\nEpoch [1800/20000] | mean loss:    4.652, time: 0:56:41",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa8f0033ebfa4381bbfaa9c474f339f8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    4.652, Val Hit@10: 41.52%, Time 0:57:44 *\nEpoch [2000/20000] | mean loss:    4.269, time: 1:03:05",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a050c10062c458199eb2f14ba22b1f5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    4.269, Val Hit@10: 42.64%, Time 1:04:08 *\nEpoch [2200/20000] | mean loss:    4.200, time: 1:09:30",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91eaed28af2341c5b957e40016a25de6"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    4.200, Val Hit@10: 43.81%, Time 1:10:33 *\nEpoch [2400/20000] | mean loss:    3.395, time: 1:15:54",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c50393ba33c4e0aa0761f3850d188b4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.395, Val Hit@10: 44.58%, Time 1:16:57 *\nEpoch [2600/20000] | mean loss:    3.344, time: 1:22:19",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a533ee5e29243238ec1121e88c4fcd4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.344, Val Hit@10: 45.16%, Time 1:23:22 *\nEpoch [2800/20000] | mean loss:    3.543, time: 1:28:44",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85394262b9a34dc7ae46b8bb2435bc79"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.543, Val Hit@10: 45.73%, Time 1:29:47 *\nEpoch [3000/20000] | mean loss:    3.614, time: 1:35:09",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64d56e0abf404f92b2e384bde38c253d"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.614, Val Hit@10: 46.06%, Time 1:36:12 *\nEpoch [3200/20000] | mean loss:    3.395, time: 1:41:35",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77535ced91f34270aae65bcdddb0c67d"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.395, Val Hit@10: 46.38%, Time 1:42:39 *\nEpoch [3400/20000] | mean loss:    3.588, time: 1:48:01",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43204d9411924709805638dbdd1ec264"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.588, Val Hit@10: 46.93%, Time 1:49:04 *\nEpoch [3600/20000] | mean loss:    3.717, time: 1:54:26",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "896faf4266594f13a546be705450fb8e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.717, Val Hit@10: 47.28%, Time 1:55:30 *\nEpoch [3800/20000] | mean loss:    3.530, time: 2:00:52",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e879cbfc1ec4267870e3c8b828d24d8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.530, Val Hit@10: 47.59%, Time 2:01:55 *\nEpoch [4000/20000] | mean loss:    3.954, time: 2:07:18",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6a4f5562f714ff782da631410b440df"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.954, Val Hit@10: 47.83%, Time 2:08:21 *\nEpoch [4200/20000] | mean loss:    2.779, time: 2:13:43",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f09850df4dc4d6c9805797d52d37091"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    2.779, Val Hit@10: 47.86%, Time 2:14:47 *\nEpoch [4400/20000] | mean loss:    3.025, time: 2:20:10",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9a26f25e5334b4faa1d67afe1dbee8b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.025, Val Hit@10: 48.16%, Time 2:21:13 *\nEpoch [4600/20000] | mean loss:    3.271, time: 2:26:35",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b92d14d00cad4cd2bfe0a122cba243f9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.271, Val Hit@10: 48.35%, Time 2:27:38 *\nEpoch [4800/20000] | mean loss:    3.057, time: 2:33:00",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ccb469e0acd4817a07c432da0290730"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.057, Val Hit@10: 48.35%, Time 2:34:03 *\nEpoch [5000/20000] | mean loss:    3.089, time: 2:39:26",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8436bb629eaf4c5b947f99ae85272fb2"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nTrain loss:    3.089, Val Hit@10: 48.70%, Time 2:40:30 *\nEpoch [5003/20000] | mean loss:    3.908, time: 2:40:35",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-667fbb47dbd1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m     \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-3-667fbb47dbd1>\u001B[0m in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     95\u001B[0m             \u001B[0mpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mneg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_h\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_t\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mneg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 97\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     98\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    130\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    133\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from torch import cuda\nfrom torch.optim import Adam\nimport torch\n\n# from torchkge.models import TransEModel\nfrom torchkge.sampling import BernoulliNegativeSampler\nfrom torchkge.utils import MarginLoss, DataLoader\n# from torchkge.utils.datasets import load_GADM9\nfrom torchkge.evaluation import LinkPredictionEvaluator\n\nfrom torchkge.utils.my_utils import load_ckpt, save_ckpt, create_dir_not_exists,time_since\n\nimport os\nfrom importlib import import_module\nfrom tqdm.autonotebook import tqdm\nimport time\n\n\ndef main():\n    # Define some hyper-parameters for training\n    model_name = 'TransR'\n    benchmarks = 'GADM9'\n\n    emb_dim = 100\n    ent_dim = emb_dim\n    rel_dim = emb_dim\n    lr = 0.0001\n    margin = 1\n\n    n_epochs = 5000\n    train_b_size = 5120  # 训练时batch size\n    eval_b_size = 128  # 测评valid test 时batch size\n    validation_freq = 200 # 多少轮进行在验证集进行一次测试 同时保存最佳模型\n    require_improvement = validation_freq*5  # 验证集top_k超过多少epoch没下降，结束训练\n    model_save_path = './checkpoint/' + benchmarks +'_'+ model_name + '_Adam.ckpt'  # 保存最佳hits k (ent)模型\n    device = 'cuda:0' if cuda.is_available() else 'cpu'\n\n    # Load dataset\n    module = getattr(import_module('torchkge.models'), model_name+'Model')\n    load_data = getattr(import_module('torchkge.utils.datasets'), 'load_'+benchmarks)\n\n    print('Loading data...')\n    kg_train, kg_val, kg_test = load_data()\n    print(f'Train set: {kg_train.n_ent} entities, {kg_train.n_rel} relations, {kg_train.n_facts} triplets.')\n    print(f'Valid set: {kg_val.n_facts} triplets, Test set: {kg_test.n_facts} triplets.')\n\n    # Define the model and criterion\n    print('Loading model...')\n    model = module(ent_dim, rel_dim, kg_train.n_ent, kg_train.n_rel)\n    # model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n    criterion = MarginLoss(margin)\n    # Move everything to CUDA if available\n    if device == 'cuda:0':\n        cuda.empty_cache()\n        model.to(device)\n        criterion.to(device)\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda='all')\n    else:\n        dataloader = DataLoader(kg_train, batch_size=train_b_size, use_cuda=None)\n\n\n    # Define the torch optimizer to be used\n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n    sampler = BernoulliNegativeSampler(kg_train)\n\n    start_epoch = 1\n    best_score = float('-inf')\n\n    if os.path.exists(model_save_path):  # 存在则加载模型 并继续训练\n        start_epoch, best_score = load_ckpt(model_save_path, model, optimizer)\n        print(f'loading ckpt sucessful, start on epoch {start_epoch}...')\n    print(model)\n    print('lr: {}, margin: {}, dim {}, total epoch: {}, device: {}, batch size: {}, optim: {}'\\\n    .format(lr, margin, emb_dim, n_epochs, device, train_b_size, optimizer))\n\n    # iterator = tqdm(range(start_epoch, n_epochs+1), unit='epoch')\n    print('Training ...')\n\n    last_improve = start_epoch  # 记录上次验证集loss下降的epoch数\n\n    start = time.time()\n    for epoch in range(start_epoch, n_epochs+1):\n        # scheduler.step()  # lr衰减\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader):\n            h, t, r = batch[0], batch[1], batch[2]\n            n_h, n_t = sampler.corrupt_batch(h, t, r)\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            pos, neg = model(h, t, n_h, n_t, r)\n            loss = criterion(pos, neg)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print('\\rEpoch [{:>4}/{:>4}] | mean loss: {:>8.3f}, time: {}'.format(epoch, n_epochs, running_loss / len(dataloader), time_since(start)), end='',flush=True)\n        # iterator.set_description(\n        #     'Epoch {:>5} | mean loss: {:>8.3f}'.format(epoch,\n        #                                           running_loss / len(dataloader)))\n        # test\n        if epoch % validation_freq == 0:\n            create_dir_not_exists('./checkpoint')\n            model.eval()\n            evaluator = LinkPredictionEvaluator(model, kg_val)\n            evaluator.evaluate(b_size=eval_b_size, verbose=False)\n            _, hit_at_k = evaluator.hit_at_k(10)  # val filter hit_k\n            if hit_at_k > best_score:\n                save_ckpt(model, optimizer, epoch, best_score, model_save_path)\n                best_score = hit_at_k\n                improve = '*'  # 在有提升的结果后面加上*标注\n                last_improve = epoch  # 验证集hit_k增大即认为有提升\n            else:\n                improve = ''\n            msg = '\\nTrain loss: {:>8.3f}, Val Hit@10: {:>5.2%}, Time {} {}'\n            print(msg.format(running_loss / len(dataloader), hit_at_k, time_since(start), improve))\n        model.normalize_parameters()\n        if epoch - last_improve > require_improvement:\n            # 验证集loss超过1000batch没下降，结束训练\n            print(\"\\nNo optimization for a long time, auto-stopping...\")\n            break\n\n    print('Training done, start evaluate on test data...')\n    # Testing the best checkpoint on test dataset\n    load_ckpt(model_save_path, model, optimizer)\n    model.eval()\n    evaluator = LinkPredictionEvaluator(model, kg_test)\n    evaluator.evaluate(eval_b_size, verbose=False)\n    evaluator.print_results()\n\n\nif __name__ == \"__main__\":\n    main()\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTrain set: 34715 entities, 9 relations, 398758 triplets.\nValid set: 49845 triplets, Test set: 49849 triplets.\nLoading model...\nloading ckpt sucessful, start on epoch 5001...\nTransRModel(\n  (ent_emb): Embedding(34715, 100)\n  (rel_emb): Embedding(9, 100)\n  (proj_mat): Embedding(9, 10000)\n)\nlr: 0.0001, margin: 1, dim 100, total epoch: 5000, device: cuda:0, batch size: 5120, optim: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.0001\n    weight_decay: 1e-05\n)\nTraining ...\nTraining done, start evaluate on test data...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Projecting entities:   0%|          | 0/34715 [00:00<?, ?entities/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfa190606401400b93719aed878f9852"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Hit@10 : 0.419 \t\t Filt. Hit@10 : 0.485\nMean Rank : 512 \t Filt. Mean Rank : 314\nMRR : 0.164 \t\t Filt. MRR : 0.205\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/Moon-xm/torchkge.git",
   "metadata": {
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'torchkge'...\nremote: Enumerating objects: 201, done.\u001B[K\nremote: Counting objects: 100% (201/201), done.\u001B[K\nremote: Compressing objects: 100% (129/129), done.\u001B[K\nremote: Total 3522 (delta 114), reused 149 (delta 71), pack-reused 3321\u001B[K\nReceiving objects: 100% (3522/3522), 74.59 MiB | 13.24 MiB/s, done.\nResolving deltas: 100% (2316/2316), done.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}